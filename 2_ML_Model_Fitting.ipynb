{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature set for set8 ml model\n",
    "s8_fset = pd.read_csv('./datasets/feature_data.csv')\n",
    "msight_scores = pd.read_csv('./datasets/MS-full-proteome.csv')\n",
    "ms_test = pd.read_csv('./datasets/MS-test-set.csv')\n",
    "pplus = pd.read_csv('./datasets/annotated_methylome.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format s8 feature set \n",
    "# Format ID (uniprotID_K#) for feature set \n",
    "s8_fset['ID'] = s8_fset['ID'].str[:-3]\n",
    "\n",
    "# Format ID (uniprotID_K#) for methylsight holdout set\n",
    "ms_test['ID'] = ms_test['uniprot_id'] + '_K' + ms_test['position'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to generate holdout test set\n",
    "holdout_set = pd.merge(ms_test, s8_fset, on = 'ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAINING AND TEST SETS FOR S8 ML MODEL\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create x and y sets for set8 ml test data\n",
    "holdout_set = holdout_set.set_index('ID')\n",
    "s8_test_y = holdout_set['METHYLATED']\n",
    "s8_test_x = holdout_set.drop(['uniprot_id', 'position', 'METHYLATED'], axis=1)\n",
    "# create x and y sets for set8 ml training data with test set removed\n",
    "drop_rows = holdout_set.index\n",
    "s8_fset = s8_fset.set_index('ID')\n",
    "s8_fset.drop(drop_rows, inplace=True)\n",
    "s8_train_y = s8_fset['METHYLATED']\n",
    "s8_train_x = s8_fset.drop(['METHYLATED'], axis=1)\n",
    "# split training sets in two for 1) training of set8 model then 2) training of combo model\n",
    "s8_mod_train_x, s8_combo_train_x, s8_mod_train_y, s8_combo_train_y = train_test_split(\n",
    "    s8_train_x, s8_train_y, test_size=0.5, random_state=8, stratify=s8_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count training and test set sizes for 1) and 2)\n",
    "from collections import Counter\n",
    "print('Hold-Out Set:', Counter(s8_test_y))\n",
    "print('Without Hold-Out Set:', Counter(s8_train_y))\n",
    "print('S8 Model Training:', Counter(s8_mod_train_y))\n",
    "print('Combo Model Training:', Counter(s8_combo_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST S8 MODEL on S8 Model Training data\n",
    "# Linear Discriminant Analysis produced the best results in testing - use it here\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from numpy import mean\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "discrim = LinearDiscriminantAnalysis(tol = 0.007070707070707071, store_covariance=False, solver='svd')\n",
    "over = RandomOverSampler(sampling_strategy=0.35)\n",
    "steps = [('sampling', over), ('model', discrim)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\n",
    "scores = cross_val_score(pipeline, s8_mod_train_x, s8_mod_train_y, cv=cv, scoring=\"f1\")\n",
    "print(\"score:\", mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN S8 MODEL\n",
    "# fit model to training data\n",
    "step2 = [('sampling', over)]\n",
    "pip2 = Pipeline(steps=step2)\n",
    "s8_X, s8_y = pip2.fit_resample(s8_mod_train_x, s8_mod_train_y)\n",
    "model = discrim.fit(s8_X, s8_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT SCORES FOR COMBO TRAINING SET USING S8 MODEL FOR INSERTION INTO COMBO MODEL\n",
    "s8_scores = model.predict_proba(s8_combo_train_x)\n",
    "combo_train_x = pd.DataFrame(s8_scores)\n",
    "combo_train_x = combo_train_x.rename(columns={1:'s8_proba_methylated'})\n",
    "combo_train_x = combo_train_x.drop([0], axis=1)\n",
    "combo_train_x = combo_train_x.set_index(s8_combo_train_x.index)\n",
    "combo_train_x['methylated'] = s8_combo_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MS SCORES TO COMBO TRAINING SET\n",
    "# formatting\n",
    "ms_combo_train_x = pd.DataFrame(msight_scores['Score'])\n",
    "msight_scores['ID'] = msight_scores['UniProtID'] + '_K' + msight_scores['Site'].astype(str)\n",
    "ms_combo_train_x = ms_combo_train_x.set_index(msight_scores['ID'])\n",
    "ms_combo_train_x = ms_combo_train_x.rename(columns={'Score':'ms_proba_methylated'})\n",
    "\n",
    "# add to s8 data\n",
    "X_combo_train = pd.merge(combo_train_x, ms_combo_train_x, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# isolate methylated data for y combo train\n",
    "y_combo_train = X_combo_train['methylated']\n",
    "X_combo_train = X_combo_train.drop(['methylated'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MethylSight and SET8ML Predicted Positives:', len(X_combo_train[(X_combo_train['ms_proba_methylated'] >= 0.5) & (X_combo_train['s8_proba_methylated'] >= 0.5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('SET8ML Predicted Positives:', len(X_combo_train[X_combo_train['s8_proba_methylated'] >= 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick count of classes in combo train\n",
    "print('Combination Model Training Set:', Counter(y_combo_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE COMBO TEST SET\n",
    "# y value is experimental methylation data\n",
    "y_combo_test = holdout_set['METHYLATED']\n",
    "# generate s8 scores for combo x data\n",
    "X_s8_combo_test = holdout_set.drop(['ground_truth', 'uniprot_id', 'METHYLATED', 'position'], axis=1)\n",
    "s8_t_scores = model.predict_proba(X_s8_combo_test)\n",
    "x_combo_test = pd.DataFrame(s8_t_scores)\n",
    "x_combo_test = x_combo_test.rename(columns={1:'s8_proba_methylated'})\n",
    "x_combo_test = x_combo_test.drop([0], axis=1)\n",
    "x_combo_test = x_combo_test.set_index(X_s8_combo_test.index)\n",
    "x_combo_test['methylated'] = y_combo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out ms scores from ms data for combo test x data\n",
    "X_combo_test = pd.merge(x_combo_test, ms_combo_train_x, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# isolate methylated data for y combo train\n",
    "y_combo_test = X_combo_test['methylated']\n",
    "X_combo_test = X_combo_test.drop(['methylated'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT AND TEST MODELS : train with X_combo_train and y_combo_train,\n",
    "# test with X_combo_test and y_combo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTING ~ Mean Probabilities\n",
    "mean_prob = (X_combo_test['s8_proba_methylated'] + X_combo_test['ms_proba_methylated'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTING ~ Sum Probabilities\n",
    "sum_prob = (X_combo_test['s8_proba_methylated'] + X_combo_test['ms_proba_methylated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH VOTING\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# METRIC v. THRESHOLD\n",
    "threshold = 0.0\n",
    "rec = []\n",
    "prec = []\n",
    "spec = []\n",
    "thresh = []\n",
    "\n",
    "while threshold <= 1:\n",
    "    \n",
    "    # select just the scores over our threshold\n",
    "    cond = mean_prob >= threshold\n",
    "    # convert to 1 or 0 values\n",
    "    y_pred = np.where((mean_prob>=threshold), 1, mean_prob)\n",
    "    y_pred = np.where((y_pred<threshold), 0, y_pred)\n",
    "    # calculate recall\n",
    "    recall = recall_score(y_combo_test, y_pred)\n",
    "    rec.append(recall)\n",
    "    # calculate precision\n",
    "    precision = precision_score(y_combo_test, y_pred)\n",
    "    prec.append(precision)\n",
    "    # calculate specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_combo_test, y_pred).ravel()\n",
    "    specificity = tn/(tn+fp)\n",
    "    spec.append(specificity)\n",
    "    \n",
    "    # update threshold\n",
    "    thresh.append(threshold)\n",
    "    threshold += 0.01\n",
    "    #print(threshold)\n",
    "\n",
    "voting_metrics = pd.DataFrame(list(zip(thresh, rec, spec, prec)), columns=['Threshold'\n",
    "                                                                              , 'Recall'\n",
    "                                                                              , 'Specificity'\n",
    "                                                                              , 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_context('paper')\n",
    "ax = sns.lineplot(x='Threshold', y='value', hue='variable', data=pd.melt(voting_metrics, 'Threshold'))\n",
    "ax.set(ylabel='Performance')\n",
    "ax.grid()\n",
    "ax.legend(title='', bbox_to_anchor=(.5, 1), loc='lower center', ncol=3)\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,1)\n",
    "plt.savefig('./mean_sum_ms_s8.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT DIFFERENT ML MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append feature set to pdcal set then run through model fitting\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "# Create our models\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "logit = LogisticRegression(max_iter=100)\n",
    "nb = ComplementNB()\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "svc = svm.SVC()\n",
    "bagged = BaggingClassifier()\n",
    "rand_forest = RandomForestClassifier()\n",
    "ext_trees = ExtraTreesClassifier()\n",
    "gboost = GradientBoostingClassifier()\n",
    "\n",
    "models = [dummy, logit, nb, dtree, knn, svc, bagged, rand_forest, ext_trees, gboost]\n",
    "\n",
    "\n",
    "for m in models:\n",
    "    mod = m.fit(X_combo_train, y_combo_train)\n",
    "    predicted_y = mod.predict(X_combo_test)\n",
    "    print('Model_Type:', m, '\\t f1:', precision_score(y_combo_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb + dtree + bagged gives highest score - proceed to sampling strategies with that\n",
    "# OVERSAMPLING\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, KMeansSMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "#models = [nb, dtree, bagged]\n",
    "\n",
    "# Create our oversamplers\n",
    "over = RandomOverSampler()\n",
    "smote = SMOTE()\n",
    "border_smote = BorderlineSMOTE()\n",
    "svm_smote = SVMSMOTE()\n",
    "km_smote = KMeansSMOTE()\n",
    "adasyn = ADASYN()\n",
    "samplings = [over, smote, border_smote, svm_smote, adasyn]   #all oversamplers\n",
    "\n",
    "top = 0\n",
    "topmodel = ''\n",
    "stype = ''\n",
    "\n",
    "for m in models:\n",
    "    for s in samplings:\n",
    "        s3 = [('sampling', s), ('clf', m)]\n",
    "        pip3 = Pipeline(steps=s3)\n",
    "        m3 = pip3.fit(X_combo_train, y_combo_train)\n",
    "        #re_X, re_y = pip3.fit_resample(X_combo_train, y_combo_train)\n",
    "        #m3 = m.fit(re_X, re_y)\n",
    "        predicted_y = m3.predict(X_combo_test)\n",
    "        f1 = f1_score(y_combo_test, predicted_y)\n",
    "        if f1 > top:\n",
    "            top = f1\n",
    "            topmodel=m\n",
    "            stype=s\n",
    "        #print('Model_Type:', m, '\\t Sampling_type:', s ,'\\t f1:', f1_score(y_combo_test, predicted_y))\n",
    "print('Top scoring model is', topmodel, 'with', stype, 'sampling, and an f1 score of', top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDERSAMPLING\n",
    "# Data Undersampling - random undersampling, condensed nearest neighbor, tomek links,\n",
    "# edited nearest neighbors, neighborhood cleaning rule, one-sided selection\n",
    "from imblearn.under_sampling import RandomUnderSampler, CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule, OneSidedSelection\n",
    "\n",
    "under = RandomUnderSampler()\n",
    "cnn = CondensedNearestNeighbour()\n",
    "tomek = TomekLinks()\n",
    "enn = EditedNearestNeighbours()\n",
    "n_cleaning = NeighbourhoodCleaningRule()\n",
    "onesided = OneSidedSelection()\n",
    "u_samplings = [under, cnn, tomek, enn, n_cleaning, onesided]\n",
    "\n",
    "top = 0\n",
    "topmodel = ''\n",
    "stype = ''\n",
    "\n",
    "for m in models:\n",
    "    for u in u_samplings:\n",
    "        s4 = [('sampling', u)]\n",
    "        pip4 = Pipeline(steps=s4)\n",
    "        u_re_X, u_re_y = pip4.fit_resample(X_combo_train, y_combo_train)\n",
    "        m4 = m.fit(u_re_X, u_re_y)\n",
    "        predicted_y = m4.predict(X_combo_test)\n",
    "        f1 = f1_score(y_combo_test, predicted_y)\n",
    "        if f1 > top:\n",
    "            top = f1\n",
    "            topmodel=m\n",
    "            stype=s\n",
    "print('Top scoring model is', topmodel, 'with', stype, 'sampling, and an f1 score of', top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERSAMPLING + UNDERSAMPLING\n",
    "# Combined Oversampling + Undersampling - smote + random undersampling, smote + tomek links, \n",
    "# smote + edited nearest neighbors\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "smoteenn = SMOTEENN()\n",
    "smotetomek = SMOTETomek()\n",
    "combined = [smoteenn, smotetomek]\n",
    "\n",
    "top = 0\n",
    "topmodel = ''\n",
    "stype = ''\n",
    "\n",
    "for m in models:\n",
    "    for c in combined:\n",
    "        s5 = [('sampling', c)]\n",
    "        pip5 = Pipeline(steps=s5)\n",
    "        c_re_X, c_re_y = pip5.fit_resample(X_combo_train, y_combo_train)\n",
    "        m5 = m.fit(c_re_X, c_re_y)\n",
    "        predicted_y = m5.predict(X_combo_test)\n",
    "        f1 = f1_score(y_combo_test, predicted_y)\n",
    "        if f1 > top:\n",
    "            top = f1\n",
    "            topmodel=m\n",
    "            stype=c\n",
    "print('Top scoring model is', topmodel, 'with', stype, 'sampling, and an f1 score of', top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER SEARCH - cannot be run on ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Logistic Regression + SVMSMOTE oversampling produced highest score - proceed with those\n",
    "svm_smote = SVMSMOTE()\n",
    "steps = [('sampling', svm_smote), ('clf', logit)]\n",
    "pipe = Pipeline(steps=steps)\n",
    "# fit the model to the training data\n",
    "f_model = pipe.fit(X_combo_train, y_combo_train)\n",
    "\n",
    "# GRAPHING: METRIC v. THRESHOLD\n",
    "threshold = 0.0\n",
    "t_rec = []\n",
    "t_prec = []\n",
    "t_spec = []\n",
    "thresh = []\n",
    "\n",
    "while threshold <= 1:\n",
    "    # calculate probability on testing data\n",
    "    y_proba = f_model.predict_proba(X_combo_test)\n",
    "    y_proba = y_proba[:, [1]]   #select the probability for the positive case only\n",
    "    \n",
    "    # select just the scores over our threshold\n",
    "    cond = y_proba >= threshold\n",
    "    # convert to 1 or 0 values\n",
    "    y_pred = np.where((y_proba>=threshold), 1, y_proba)\n",
    "    y_pred = np.where((y_pred<threshold), 0, y_pred)\n",
    "    # calculate recall\n",
    "    recall = recall_score(y_combo_test, y_pred)\n",
    "    t_rec.append(recall)\n",
    "    # calculate precision\n",
    "    precision = precision_score(y_combo_test, y_pred)\n",
    "    t_prec.append(precision)\n",
    "    # calculate specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_combo_test, y_pred).ravel()\n",
    "    specificity = tn/(tn+fp)\n",
    "    t_spec.append(specificity)\n",
    "\n",
    "    # update threshold\n",
    "    thresh.append(threshold)\n",
    "    threshold += 0.01\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(list(zip(thresh, t_rec, t_spec, t_prec)), columns=['Threshold'\n",
    "                                                                              , 'Recall'\n",
    "                                                                              , 'Specificity'\n",
    "                                                                              , 'Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph it\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_context('paper')\n",
    "ax = sns.lineplot(x='Threshold', y='value', hue='variable', data=pd.melt(metrics, 'Threshold'))\n",
    "ax.set(ylabel='Performance')\n",
    "ax.grid()\n",
    "ax.legend(title='', bbox_to_anchor=(.5, 1), loc='lower center', ncol=3)\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,1)\n",
    "plt.savefig('./combined_ms_s8_logistic_regression_svmsmote.pdf', dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
